{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_concat = pd.read_csv(\"../data_preprocessed/train_concat.csv.gz\")\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat = train_concat.drop(['Unnamed: 0', 'sWeight', 'kinWeight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = ['ncl[0]', 'ncl[1]', 'ncl[2]', 'ncl[3]', 'avg_cs[0]', 'avg_cs[1]',\n",
    "       'avg_cs[2]', 'avg_cs[3]', 'ndof', 'MatchedHit_X[0]', 'MatchedHit_X[1]',\n",
    "       'MatchedHit_X[2]', 'MatchedHit_X[3]', 'MatchedHit_Y[0]',\n",
    "       'MatchedHit_Y[1]', 'MatchedHit_Y[2]', 'MatchedHit_Y[3]',\n",
    "       'MatchedHit_Z[0]', 'MatchedHit_Z[1]', 'MatchedHit_Z[2]',\n",
    "       'MatchedHit_Z[3]', 'MatchedHit_DX[0]', 'MatchedHit_DX[1]',\n",
    "       'MatchedHit_DX[2]', 'MatchedHit_DX[3]', 'MatchedHit_DY[0]',\n",
    "       'MatchedHit_DY[1]', 'MatchedHit_DY[2]', 'MatchedHit_DY[3]',\n",
    "       'MatchedHit_DZ[0]', 'MatchedHit_DZ[1]', 'MatchedHit_DZ[2]',\n",
    "       'MatchedHit_DZ[3]', 'MatchedHit_T[0]', 'MatchedHit_T[1]',\n",
    "       'MatchedHit_T[2]', 'MatchedHit_T[3]', 'MatchedHit_DT[0]',\n",
    "       'MatchedHit_DT[1]', 'MatchedHit_DT[2]', 'MatchedHit_DT[3]',\n",
    "       'Lextra_X[0]', 'Lextra_X[1]', 'Lextra_X[2]', 'Lextra_X[3]',\n",
    "       'Lextra_Y[0]', 'Lextra_Y[1]', 'Lextra_Y[2]', 'Lextra_Y[3]', 'NShared',\n",
    "       'Mextra_DX2[0]', 'Mextra_DX2[1]', 'Mextra_DX2[2]', 'Mextra_DX2[3]',\n",
    "       'Mextra_DY2[0]', 'Mextra_DY2[1]', 'Mextra_DY2[2]', 'Mextra_DY2[3]',\n",
    "       'FOI_hits_N', 'PT', 'P', '0', '1', '2', '3', '4',\n",
    "       '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17',\n",
    "       '18', '19', '20', '21', '22', '23']\n",
    "cat_features = [ 'MatchedHit_TYPE[0]', 'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]', 'MatchedHit_TYPE[3]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat['PT_P'] = train_concat.P - train_concat.PT\n",
    "train_features.append('PT_P')\n",
    "train_concat['PT_P/P'] = train_concat.PT_P / train_concat.P\n",
    "train_features.append('PT_P/P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_missing(data):\n",
    "    missing = False\n",
    "    values = []\n",
    "    for i in data:\n",
    "        if i == -9999:\n",
    "            missing = True\n",
    "        else:\n",
    "            missing = False\n",
    "        values.append(missing)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_features = []\n",
    "nans = []\n",
    "for i in ['MatchedHit_X[0]', 'MatchedHit_X[1]', 'MatchedHit_X[2]', 'MatchedHit_X[3]', \n",
    "          'MatchedHit_Y[0]', 'MatchedHit_Y[1]', 'MatchedHit_Y[2]', 'MatchedHit_Y[3]']:\n",
    "    train_concat['abs_%s' %i] = abs(train_concat['%s' %i])\n",
    "    added_features.append('abs_%s' %i)\n",
    "    nans.append('nan_%s' %i)\n",
    "    train_concat['nan_%s' %i] = has_missing(train_concat['%s' %i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat = pd.get_dummies(train_concat, columns = cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ['MatchedHit_TYPE[0]_1',\n",
    "       'MatchedHit_TYPE[0]_2', 'MatchedHit_TYPE[1]_1', 'MatchedHit_TYPE[1]_2',\n",
    "       'MatchedHit_TYPE[2]_0', 'MatchedHit_TYPE[2]_1', 'MatchedHit_TYPE[2]_2',\n",
    "       'MatchedHit_TYPE[3]_0', 'MatchedHit_TYPE[3]_1', 'MatchedHit_TYPE[3]_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concat = pd.read_csv(\"../data_preprocessed/private_concat.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concat['PT_P'] = test_concat.P - test_concat.PT\n",
    "test_concat['PT_P/P'] = test_concat.PT_P/test_concat.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['MatchedHit_X[0]', 'MatchedHit_X[1]', 'MatchedHit_X[2]', 'MatchedHit_X[3]', \n",
    "          'MatchedHit_Y[0]', 'MatchedHit_Y[1]', 'MatchedHit_Y[2]', 'MatchedHit_Y[3]']:\n",
    "    test_concat['abs_%s' %i] = abs(test_concat['%s' %i])\n",
    "    test_concat['nan_%s' %i] = has_missing(test_concat['%s' %i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concat = pd.get_dummies(test_concat, columns = cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_test = ['MatchedHit_TYPE[0]_1.0', 'MatchedHit_TYPE[0]_2.0',\n",
    "       'MatchedHit_TYPE[1]_1.0', 'MatchedHit_TYPE[1]_2.0',\n",
    "       'MatchedHit_TYPE[2]_0.0', 'MatchedHit_TYPE[2]_1.0',\n",
    "       'MatchedHit_TYPE[2]_2.0', 'MatchedHit_TYPE[3]_0.0',\n",
    "       'MatchedHit_TYPE[3]_1.0', 'MatchedHit_TYPE[3]_2.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "kf = KFold(n_splits = 5, random_state = 41, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "validation_predictions = []\n",
    "test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(KFold, cat_features, has_missing, i, pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(train_concat):\n",
    "    print(train_index, test_index)\n",
    "    X_train, X_val = train_concat.loc[train_index], train_concat.loc[test_index]\n",
    "    y_train, y_val = train_concat.label[train_index], train_concat.label[test_index]\n",
    "    train_data = lgb.Dataset(X_train[train_features+added_features+dummies+nans], \n",
    "                         label = y_train, weight=X_train.weight, \n",
    "                         feature_name= train_features+added_features+dummies+nans, \n",
    "                         categorical_feature=dummies+nans)\n",
    "    param = {'random_state':42, 'max_depth':9, 'objective':'binary', 'learning_rate':0.2,'num_leaves':128,'min_data_in_leaf':15, 'num_iterations':90}\n",
    "    model = lgb.train(param, train_data)                        \n",
    "    prediction = model.predict(X_val[train_features+added_features+dummies+nans])\n",
    "    validation_predictions.append(prediction)\n",
    "    prediction_test = model.predict(test_concat[train_features+added_features+dummies_test+nans])\n",
    "    test_predictions.append(list(prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validations = []\n",
    "for sublist in validation_predictions:\n",
    "    for item in sublist:\n",
    "        validations.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "for i in range(len(test_predictions[0])):\n",
    "    sum=0\n",
    "    for j in range(5):\n",
    "        sum+=test_predictions[j][i]\n",
    "    test.append(sum / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"../predictions/lgbm_dummies_val\", validations)\n",
    "np.savetxt(\"../predictions/lgbm_dummies_private\", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
